{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:27:01.843307Z",
     "start_time": "2021-03-31T06:27:01.835310Z"
    },
    "id": "iAQPFiNQ3tij"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:26:35.675790Z",
     "start_time": "2021-03-31T06:26:00.108676Z"
    },
    "id": "c_aKoLjm4xjP",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_4vzCYb3tij"
   },
   "source": [
    "#### What is Tokenization?\n",
    "A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. Tokenization is the process of splitting a string into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:29:57.218389Z",
     "start_time": "2021-03-31T06:29:57.210610Z"
    },
    "id": "NdbAv3gj3tik",
    "outputId": "f2eb3514-9ddc-49ff-e79d-2589062d21d2"
   },
   "outputs": [],
   "source": [
    "mystring = \"My favorite color is blue\"\n",
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:34:41.015269Z",
     "start_time": "2021-03-31T06:34:40.999613Z"
    },
    "id": "7OgpLxRk3tik",
    "outputId": "196f718e-925c-412c-ed29-46887abe6349"
   },
   "outputs": [],
   "source": [
    "mystring = \"My favorite colors are blue, red, and green.\"\n",
    "mystring.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEPntkBb3til"
   },
   "source": [
    "the punctuation marks are grouped in with their adjacent word (e.g. blue,). This is problematic for NLP applications, as the goal of tokenization is generally to divide a set (corpus) of documents into a common set of building blocks that can then be used as a basis for comparison. Hence, it’s no good if “blue” in \"My favorite color is blue\" doesn’t match with “blue” in \"My favorite colors are blue, red, and green.\" since the latter is tokenized as blue, rather than blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:34:44.229352Z",
     "start_time": "2021-03-31T06:34:44.213359Z"
    },
    "id": "ZyHvm2v13til",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:34:44.640054Z",
     "start_time": "2021-03-31T06:34:44.600048Z"
    },
    "id": "wCrXo3zD3til",
    "outputId": "918ba62b-c37a-4107-f062-4ebdab03febe"
   },
   "outputs": [],
   "source": [
    "word_tokenize(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:18.337395Z",
     "start_time": "2021-03-31T06:36:18.329701Z"
    },
    "id": "kh9GCoac3til",
    "outputId": "b0f41cc6-759a-4e68-c438-1328074b2700"
   },
   "outputs": [],
   "source": [
    "sentence = \"My name is George and I love NLP\"\n",
    "token1 = word_tokenize(sentence)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:10.849238Z",
     "start_time": "2021-03-31T06:40:10.833424Z"
    },
    "id": "yIY4Bw2z3til",
    "outputId": "20ea900f-d17b-459e-bb6f-fb2f61c24250"
   },
   "outputs": [],
   "source": [
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:11.220586Z",
     "start_time": "2021-03-31T06:40:11.204828Z"
    },
    "id": "zxbS_9Tk3tim",
    "outputId": "9d58fe4c-b41f-42a4-9618-0c0096ea200d"
   },
   "outputs": [],
   "source": [
    "sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:17.789265Z",
     "start_time": "2021-03-31T06:40:17.781270Z"
    },
    "id": "WSUeicHD3tim",
    "outputId": "95e989a3-9af5-4cfe-a162-7529bd94b938"
   },
   "outputs": [],
   "source": [
    "print(word_tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:19.771680Z",
     "start_time": "2021-03-31T06:40:19.713371Z"
    },
    "id": "kQREzond4xjh"
   },
   "outputs": [],
   "source": [
    "filename ='FP.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:42:29.165162Z",
     "start_time": "2021-03-31T06:42:29.141611Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "MzaLR8XW4xji",
    "outputId": "a89fbe05-bd11-48e2-bff6-e928c38d2460"
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:44:35.835847Z",
     "start_time": "2021-03-31T06:44:35.819856Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsYXl8Wz4xjj",
    "outputId": "132d92d1-b539-4f68-a441-be5720c680a3"
   },
   "outputs": [],
   "source": [
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:44:35.835847Z",
     "start_time": "2021-03-31T06:44:35.819856Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxml2C2T3tin",
    "outputId": "132d92d1-b539-4f68-a441-be5720c680a3"
   },
   "outputs": [],
   "source": [
    "print('\\n total sentences: ',len(sentences), ', last sentence : ' ,sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:44:44.681586Z",
     "start_time": "2021-03-31T06:44:44.665585Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZSB1vt_4xjk",
    "outputId": "7bcf30ab-4a61-43f0-edde-fbd702042495"
   },
   "outputs": [],
   "source": [
    "word_tokenize(sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:44:53.175993Z",
     "start_time": "2021-03-31T06:44:53.152008Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yiq2rytG4xjk",
    "outputId": "556953c6-c29d-4f3f-bd25-dfd72ed6e89d"
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(len(tokens), tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T08:25:49.782963Z",
     "start_time": "2021-03-06T08:25:49.775960Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXGrrdYX4xjl",
    "outputId": "9613f380-cf4a-4d0a-96b5-8800052f8c4d"
   },
   "outputs": [],
   "source": [
    "words = [word.lower() for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T08:25:53.005868Z",
     "start_time": "2021-03-06T08:25:52.999359Z"
    },
    "id": "i9plsoVC4xjl",
    "outputId": "432cb473-faf9-4f0d-8563-eabcc60000b6"
   },
   "outputs": [],
   "source": [
    "words2 = [word for word in tokens if word.isnumeric()]\n",
    "print(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:50:26.881684Z",
     "start_time": "2021-03-31T06:50:26.865927Z"
    },
    "id": "1Jd_L3nG3tin",
    "outputId": "b68eb50a-956c-4562-85e3-bcc849b70844"
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"@john lol that was #awesome :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM85K9Rl3tio"
   },
   "source": [
    "most likely that we’d prefer for\n",
    "\n",
    "- @ and john to be tokenized together as @john,\n",
    "- \\#  and awesome to be tokenized together as #awesome.\n",
    "This is because we’d expect that word usage in the context of hastags or at-mentions is likely different from usage in plain text.\n",
    "\n",
    "we would prefer that : and ) to be tokenized together as :), as :) is certainly more informative (e.g. for sentiment analysis) than the sum of its parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:55:24.362413Z",
     "start_time": "2021-03-31T06:55:24.346384Z"
    },
    "id": "a8hNQdrC3tio",
    "outputId": "351af2c4-2791-43f7-ed82-661b7a86ba8e"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "tweet_tokenizer.tokenize(\"@john lol that was #awesome :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:51:52.882344Z",
     "start_time": "2021-03-31T06:51:52.796250Z"
    },
    "id": "J-Fh8aUy3tio",
    "outputId": "09ae5108-756f-402c-b272-f19bf7437df6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the Brown Corpus\n",
    "from nltk.corpus import brown\n",
    "# nltk.download('brown')\n",
    "\n",
    "# total categories\n",
    "print('Total Categories:', len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:51:53.242487Z",
     "start_time": "2021-03-31T06:51:53.226500Z"
    },
    "id": "GFgvVC3A3tio",
    "outputId": "4eae1947-bb77-477d-c74c-4a2f6d363b39"
   },
   "outputs": [],
   "source": [
    "# print the categories\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:52:13.351835Z",
     "start_time": "2021-03-31T06:52:13.276454Z"
    },
    "id": "YjW6v-893tip",
    "outputId": "7e1b158e-9789-43c0-da7b-4c1debbb6f98"
   },
   "outputs": [],
   "source": [
    "# tokenized sentences\n",
    "brown.sents(categories='mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:52:29.974814Z",
     "start_time": "2021-03-31T06:52:29.685910Z"
    },
    "id": "xyU9R-3Z3tip",
    "outputId": "5ccec30f-6f38-499a-eeea-1837a2f7a54e"
   },
   "outputs": [],
   "source": [
    "# get sentences in natural form\n",
    "sentences = brown.sents(categories='mystery')\n",
    "sentences = [' '.join(sentence_token) for sentence_token in sentences]\n",
    "sentences[0:5] # viewing the first 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybZarNSM3tip"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "01. Tokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
